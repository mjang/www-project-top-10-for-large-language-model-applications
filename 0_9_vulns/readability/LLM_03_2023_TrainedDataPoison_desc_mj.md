The starting point of any machine learning approach is training data. When training large language models, the data is just “raw text”. The most capable LLMs train on text which covers a broad range of domains, languages, and more.

With deep neural networks, LLMs can generate responses from the patterns learned from its data.

An attacker can poison training data by:

- Manipulating the data
- Fine-tuning procedures of an LLM to introduce vulnerabilities, backdoors, or biases

Such poisoning may compromise the model’s security, effectiveness, or ethical behavior.

This relates to  LLM06:2023 - Overreliance on LLM-generated Content,  It can impact and influence users, as a chained risk vector.

Even if users do not trust the AI, this vulnerability can still hurt the reputation of the LLM.
