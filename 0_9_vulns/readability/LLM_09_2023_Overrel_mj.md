Overreliance is when systems depend too much on LLMs to make decisions or generate content.

We know that LLMs can have "hallucinations," when they make up factually incorrect and inappropriate content. Such content can lead to: 
- Misinformation
- Miscommunication
- Potential legal issues
- Damage to reputation

Solutions include adequate oversight, validation mechanisms, and clear statements of risk.
